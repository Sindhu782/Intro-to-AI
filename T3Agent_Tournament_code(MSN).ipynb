{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de1df1b-94cc-4ed0-963e-35fdf7f4a028",
   "metadata": {},
   "source": [
    "## What it does"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdac7ca-545b-4ca6-aae8-55997cd77229",
   "metadata": {},
   "source": [
    "\"This code simulates a Tic-Tac-Toe game where two AI agents compete using different strategies like Minimax, Alpha-Beta Pruning, Expectimax, and Q-learning. The goal is to demonstrate how AI strategies make decisions and play the game optimally.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19f05d-e20b-4c85-8246-2680bd20c7ac",
   "metadata": {},
   "source": [
    "## Strategies used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a3457-a5f5-4f9e-96d4-562de329922d",
   "metadata": {},
   "source": [
    "Minimax: \n",
    "A basic algorithm that recursively explores all possible moves to maximize the current player's score and minimize the opponent's score. It returns the best move based on this exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd59fd3-0dd1-41c7-aec5-cb12253504e6",
   "metadata": {},
   "source": [
    "Alpha-Beta Pruning:\n",
    "An optimized version of the Minimax algorithm that prunes branches of the search tree, reducing the number of nodes evaluated and speeding up the decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d9ddd-511e-43f1-a6fa-244cdf59830a",
   "metadata": {},
   "source": [
    "Expectimax: \n",
    "Similar to Minimax but accounts for probabilistic outcomes, averaging the possible results when it's the opponent's turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb24763-5f88-44d2-a6cf-55ea5773c11d",
   "metadata": {},
   "source": [
    "Q-Learning:\n",
    "A reinforcement learning algorithm where an agent learns the optimal strategy by updating a Q-table based on experiences (state, action, reward, next state). The agent can explore new moves or exploit known moves based on an exploration rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e052c30-6473-4eba-8b0d-0b0d1126d2b7",
   "metadata": {},
   "source": [
    "## Tic Tac Toe Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "829cde5f-a2c3-4cce-8fd2-aa3c4b207a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Tic-Tac-Toe AI Tournament!\n",
      "Available strategies: Minimax, AlphaBeta, Expectimax, QLearning\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter name for Agent 1:  e\n",
      "Enter strategy for Agent 1 (minimax, alphabeta, expectimax, qlearning):  qlearning\n",
      "Enter name for Agent 2:  r\n",
      "Enter strategy for Agent 2 (minimax, alphabeta, expectimax, qlearning):  expectimax\n",
      "Enter the number of games to simulate:  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "| X |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | O |\n",
      "-------------\n",
      "| O | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   | X |\n",
      "-------------\n",
      "|   |   |   |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | X |\n",
      "-------------\n",
      "| O | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   | X |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   | X |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "| X |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | X | O |\n",
      "-------------\n",
      "| O | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   | X |   |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "| X |   |   |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   | X |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   | X |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "| X |   |   |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "| X |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | O |\n",
      "-------------\n",
      "| O | X | X |\n",
      "It's a draw!\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   | X |   |\n",
      "| O | O | X |\n",
      "-------------\n",
      "| X | O | O |\n",
      "-------------\n",
      "| X | X | X |\n",
      "It's a draw!\n",
      "\n",
      "Tournament Results:\n",
      "{'Agent 1 Wins': 0, 'Agent 2 Wins': 0, 'Draws': 12}\n",
      "Average game duration: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# TicTacToeGame class handles the game mechanics and rules\n",
    "class TicTacToeGame:\n",
    "    def __init__(self):\n",
    "        # Initialize the game board with empty spaces\n",
    "        self.board = [' '] * 9\n",
    "\n",
    "    def display_board(self):\n",
    "        # Display the current state of the board\n",
    "        for i in range(0, 9, 3):\n",
    "            print(f\"| {self.board[i]} | {self.board[i+1]} | {self.board[i+2]} |\")\n",
    "            if i < 6:\n",
    "                print(\"-------------\")\n",
    "\n",
    "    def is_winner(self, marker):\n",
    "        # Check if the given marker (X or O) has a winning combination\n",
    "        winning_combinations = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # rows\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # columns\n",
    "            [0, 4, 8], [2, 4, 6]             # diagonals\n",
    "        ]\n",
    "        return any(all(self.board[pos] == marker for pos in combo) for combo in winning_combinations)\n",
    "\n",
    "    def is_draw(self):\n",
    "        # Check if the game has ended in a draw (no empty spaces left)\n",
    "        return all(spot != ' ' for spot in self.board)\n",
    "\n",
    "    def valid_moves(self):\n",
    "        # Return a list of all valid positions where a move can be made\n",
    "        return [i for i, spot in enumerate(self.board) if spot == ' ']\n",
    "\n",
    "    def make_move(self, position, marker):\n",
    "        # Place the marker (X or O) at the specified position if valid\n",
    "        if self.board[position] == ' ':\n",
    "            self.board[position] = marker\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# Agent class represents an AI player with a specific strategy\n",
    "class Agent:\n",
    "    def __init__(self, name, strategy):\n",
    "        # Initialize the agent with a name and a chosen strategy\n",
    "        self.name = name\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def choose_move(self, game, marker):\n",
    "        # Determine the agent's next move based on its strategy\n",
    "        if self.strategy == \"minimax\":\n",
    "            return self.minimax_move(game, marker)\n",
    "        elif self.strategy == \"alphabeta\":\n",
    "            return self.alphabeta_move(game, marker)\n",
    "        elif self.strategy == \"expectimax\":\n",
    "            return self.expectimax_move(game, marker)\n",
    "        elif self.strategy == \"qlearning\":\n",
    "            return self.q_learning_move(game, marker)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "\n",
    "    def minimax_move(self, game, marker):\n",
    "        # Use the Minimax algorithm to calculate the optimal move\n",
    "        opponent_marker = 'O' if marker == 'X' else 'X'\n",
    "        best_score = -float('inf')\n",
    "        best_move = None\n",
    "\n",
    "        for move in game.valid_moves():\n",
    "            # Simulate the move\n",
    "            game.make_move(move, marker)\n",
    "            # Evaluate the move using the Minimax algorithm\n",
    "            score = self.minimax(game, opponent_marker, False)\n",
    "            # Undo the move\n",
    "            game.make_move(move, ' ')\n",
    "            # Update the best move if the score is higher\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_move = move\n",
    "\n",
    "        return best_move\n",
    "\n",
    "    def minimax(self, game, marker, is_maximizing):\n",
    "        # Recursive Minimax function to evaluate the game state\n",
    "        opponent_marker = 'O' if marker == 'X' else 'X'\n",
    "\n",
    "        # Base cases: check for win, loss, or draw\n",
    "        if game.is_winner(marker):\n",
    "            return 1\n",
    "        if game.is_winner(opponent_marker):\n",
    "            return -1\n",
    "        if game.is_draw():\n",
    "            return 0\n",
    "\n",
    "        # Recursive case: maximize or minimize the score\n",
    "        if is_maximizing:\n",
    "            best_score = -float('inf')\n",
    "            for move in game.valid_moves():\n",
    "                game.make_move(move, marker)\n",
    "                score = self.minimax(game, opponent_marker, False)\n",
    "                game.make_move(move, ' ')\n",
    "                best_score = max(best_score, score)\n",
    "            return best_score\n",
    "        else:\n",
    "            best_score = float('inf')\n",
    "            for move in game.valid_moves():\n",
    "                game.make_move(move, marker)\n",
    "                score = self.minimax(game, opponent_marker, True)\n",
    "                game.make_move(move, ' ')\n",
    "                best_score = min(best_score, score)\n",
    "            return best_score\n",
    "\n",
    "    def alphabeta_move(self, game, marker):\n",
    "        # Use Alpha-Beta Pruning to improve the Minimax algorithm\n",
    "        opponent_marker = 'O' if marker == 'X' else 'X'\n",
    "        best_score = -float('inf')\n",
    "        best_move = None\n",
    "\n",
    "        for move in game.valid_moves():\n",
    "            # Simulate the move\n",
    "            game.make_move(move, marker)\n",
    "            # Evaluate the move using the Alpha-Beta algorithm\n",
    "            score = self.alphabeta(game, opponent_marker, -float('inf'), float('inf'), False)\n",
    "            # Undo the move\n",
    "            game.make_move(move, ' ')\n",
    "            # Update the best move if the score is higher\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_move = move\n",
    "\n",
    "        return best_move\n",
    "\n",
    "    def alphabeta(self, game, marker, alpha, beta, is_maximizing):\n",
    "        # Recursive Alpha-Beta Pruning function to evaluate the game state\n",
    "        opponent_marker = 'O' if marker == 'X' else 'X'\n",
    "\n",
    "        # Base cases: check for win, loss, or draw\n",
    "        if game.is_winner(marker):\n",
    "            return 1\n",
    "        if game.is_winner(opponent_marker):\n",
    "            return -1\n",
    "        if game.is_draw():\n",
    "            return 0\n",
    "\n",
    "        if is_maximizing:\n",
    "            best_score = -float('inf')\n",
    "            for move in game.valid_moves():\n",
    "                game.make_move(move, marker)\n",
    "                score = self.alphabeta(game, opponent_marker, alpha, beta, False)\n",
    "                game.make_move(move, ' ')\n",
    "                best_score = max(best_score, score)\n",
    "                alpha = max(alpha, best_score)\n",
    "                if beta <= alpha:\n",
    "                    break  # Beta cutoff\n",
    "            return best_score\n",
    "        else:\n",
    "            best_score = float('inf')\n",
    "            for move in game.valid_moves():\n",
    "                game.make_move(move, marker)\n",
    "                score = self.alphabeta(game, opponent_marker, alpha, beta, True)\n",
    "                game.make_move(move, ' ')\n",
    "                best_score = min(best_score, score)\n",
    "                beta = min(beta, best_score)\n",
    "                if beta <= alpha:\n",
    "                    break  # Alpha cutoff\n",
    "            return best_score\n",
    "\n",
    "    def expectimax_move(self, game, marker):\n",
    "        # Use the Expectimax algorithm to calculate the optimal move\n",
    "        opponent_marker = 'O' if marker == 'X' else 'X'\n",
    "        best_score = -float('inf')\n",
    "        best_move = None\n",
    "\n",
    "        for move in game.valid_moves():\n",
    "            game.make_move(move, marker)\n",
    "            score = self.expectimax(game, opponent_marker, False)\n",
    "            game.make_move(move, ' ')\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_move = move\n",
    "\n",
    "        return best_move\n",
    "\n",
    "    def expectimax(self, game, marker, is_maximizing):\n",
    "        # Recursive Expectimax function to evaluate the game state\n",
    "        opponent_marker = 'O' if marker == 'X' else 'X'\n",
    "\n",
    "        if game.is_winner(marker):\n",
    "            return 1\n",
    "        if game.is_winner(opponent_marker):\n",
    "            return -1\n",
    "        if game.is_draw():\n",
    "            return 0\n",
    "\n",
    "        if is_maximizing:\n",
    "            best_score = -float('inf')\n",
    "            for move in game.valid_moves():\n",
    "                game.make_move(move, marker)\n",
    "                score = self.expectimax(game, opponent_marker, False)\n",
    "                game.make_move(move, ' ')\n",
    "                best_score = max(best_score, score)\n",
    "            return best_score\n",
    "        else:\n",
    "            total_score = 0\n",
    "            valid_moves = game.valid_moves()\n",
    "            for move in valid_moves:\n",
    "                game.make_move(move, opponent_marker)\n",
    "                score = self.expectimax(game, marker, True)\n",
    "                game.make_move(move, ' ')\n",
    "                total_score += score\n",
    "            return total_score / len(valid_moves)\n",
    "\n",
    "    def q_learning_move(self, game, marker, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0):\n",
    "        # Q-learning move selection and learning\n",
    "        state = tuple(game.board)  # The current state represented as a tuple of board positions\n",
    "        if random.uniform(0, 1) < exploration_rate:\n",
    "            # Explore: choose a random move\n",
    "            return random.choice(game.valid_moves())\n",
    "        else:\n",
    "            # Exploit: choose the best move based on Q-table\n",
    "            q_values = {action: self.q_table.get((state, action), 0) for action in game.valid_moves()}\n",
    "            best_action = max(q_values, key=q_values.get)\n",
    "            return best_action\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, learning_rate=0.1, discount_factor=0.9):\n",
    "        # Update the Q-value using the Q-learning formula\n",
    "        old_q_value = self.q_table.get((state, action), 0)\n",
    "        future_q_value = max(self.q_table.get((next_state, a), 0) for a in game.valid_moves())\n",
    "        new_q_value = old_q_value + learning_rate * (reward + discount_factor * future_q_value - old_q_value)\n",
    "        self.q_table[(state, action)] = new_q_value\n",
    "\n",
    "\n",
    "# Function to simulate a single game between two agents\n",
    "def play_game(agent1, agent2):\n",
    "    game = TicTacToeGame()  # Create a new game instance\n",
    "    current_agent, current_marker = agent1, 'X'\n",
    "    other_agent, other_marker = agent2, 'O'\n",
    "\n",
    "    while True:\n",
    "        # Display the current board state\n",
    "        game.display_board()\n",
    "        # Get the current agent's move\n",
    "        move = current_agent.choose_move(game, current_marker)\n",
    "        game.make_move(move, current_marker)\n",
    "\n",
    "        # Check for a win or draw\n",
    "        if game.is_winner(current_marker):\n",
    "            game.display_board()\n",
    "            print(f\"Winner: {current_agent.name}!\")\n",
    "            return current_agent.name\n",
    "        if game.is_draw():\n",
    "            game.display_board()\n",
    "            print(\"It's a draw!\")\n",
    "            return \"Draw\"\n",
    "\n",
    "        # Switch turns\n",
    "        current_agent, other_agent = other_agent, current_agent\n",
    "        current_marker, other_marker = other_marker, current_marker\n",
    "\n",
    "\n",
    "# Main program to simulate a tournament\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the Tic-Tac-Toe AI Tournament!\")\n",
    "    print(\"Available strategies: Minimax, AlphaBeta, Expectimax, QLearning\")\n",
    "\n",
    "    # Allow the user to choose agents and strategies\n",
    "    agent1_name = input(\"Enter name for Agent 1: \")\n",
    "    agent1_strategy = input(\"Enter strategy for Agent 1 (minimax, alphabeta, expectimax, qlearning): \").lower()\n",
    "\n",
    "    agent2_name = input(\"Enter name for Agent 2: \")\n",
    "    agent2_strategy = input(\"Enter strategy for Agent 2 (minimax, alphabeta, expectimax, qlearning): \").lower()\n",
    "\n",
    "    agent1 = Agent(agent1_name, agent1_strategy)\n",
    "    agent2 = Agent(agent2_name, agent2_strategy)\n",
    "\n",
    "    # Run a tournament with multiple games\n",
    "    results = {\"Agent 1 Wins\": 0, \"Agent 2 Wins\": 0, \"Draws\": 0}\n",
    "    num_games = int(input(\"Enter the number of games to simulate: \"))\n",
    "\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_games):\n",
    "        result = play_game(agent1, agent2)\n",
    "        if result == agent1.name:\n",
    "            results[\"Agent 1 Wins\"] += 1\n",
    "        elif result == agent2.name:\n",
    "            results[\"Agent 2 Wins\"] += 1\n",
    "        else:\n",
    "            results[\"Draws\"] += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Display tournament results\n",
    "    print(\"\\nTournament Results:\")\n",
    "    print(results)\n",
    "    print(f\"Average game duration: {(end_time - start_time) / num_games:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62082872-046b-4132-b71e-191316ed4048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
